## Conclusion
{:#conclusion}
In this poster paper we explore a novel RL-based join plan optimizer based on Q-learning. We discuss key points required to translate RL-based relational optimizers to SPARQL. We will improve sampling efficiency by using experience replay and applying time-outs. Furthermore, we avoid using feature vectors that depend on the number of unique RDF entities in the RDF graph, thus restricting the size of the feature vectors. Finally, we briefly touch on unsolved problems in our current methodology and what the requirements are for approaches that solve this problem. The main problems are an absence of query shape encoding and the restriction of our method to basic graph patterns. 
