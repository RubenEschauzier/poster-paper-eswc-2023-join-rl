## Conclusion
{:#conclusion}
In this poster paper we introduce a novel reinforcement learning-based join plan optimizer based on Q-learning. We discuss some key points we believe will allow us to translate the successful reinforcement learning-based relational optimizers to SPARQL. We will improve sampling efficiency by using experience replay and applying time-outs to reduce training time. Furthermore, we avoid using feature vectors that depend on the number of unique RDF entities in the RDF graph, thus restricting the size of the feature vectors. Furthermore, we briefly touch on unsolved problems in our current methodology and what the requirements are for approaches that solve this problem. The main problems are an absence of query shape encoding and the restriction of our method to basic graph patterns. 
