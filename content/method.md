## Method
{:#method}
The RL-based optimizer builds a join plan by greedily adding joins to the join plan that minimize the estimated query execution time. We estimate the execution time of the query using a neural network, which we train to minimize the MSE between predicted and actual query execution time. The neural network is fed a numerical representation of the current join plan as an input. \\
**Join plan representation** Like in [RTOS](cite:cites yu2020reinforcement), we represent join plans as a tree that we build from the bottom up. Each leaf node represents the result set of a triple pattern, and internal nodes represent joining two result sets. We represent result sets using their cardinality, the presence and location of variables, named nodes and literals, and a vector representation of the predicate.
We learn the dense vector representations by applying [RDF2Vec](cite:cites ristoski2016rdf2vec) to the input RDF graph. The predicate vectors contain semantic information relating to the encoded predicate. 
We obtain the representations for intermediate joins by applying an N-ary Tree-LSTM on the result sets representations involved in the join. Finally, at the (partial) join plan root node, we apply a Child-Sum Tree-LSTM [operation](cite:cites tai2015improved) to all unjoined result sets to obtain the join plan representation.\\
**Data efficiency & SPARQL specific adjustments** Data generation using query execution is slow, to account for this we apply two data efficiency techniques. First, we include a time-out that is set according to existing optimizers. We effectively truncate our optimization variable, while ensuring the optimal query plan will not reach the time-out. Second, we use [experience replay](cite:cites lin1992self) to store previous (expensive) query executions and reuse them for training. \\
Relational RL-based optimization approaches use one-hot [encoding](cite:cites muller2016introduction) of database attributes to create feature vectors. However, the wikidata graph has over a 100 million unique entries. This would create unwieldy vectors, thus we do not use one-hot encoding in our approach. This complicates creating informative features but improves scalability. \\
**Open Challenges** We do not encode the connections between triple patterns, these encodings should reflect the different types of connections between triple patterns, like object-object, subject-subject, object-subject, and subject-object. Furthermore, our approach can only optimize basic graph patterns, in future work, this approach can be extended to more complex SPARQL query operations.
