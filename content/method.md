## Method
{:#method}
We will discuss our approach in three key components: featurization of join plans, model architecture and optimization target, and data efficiency improvements.

### Featurzation of (partial) join plans
{:#featurization} 
Like in [RTOS](cite:cites yu2020reinforcement), we represent join plans as a tree structure that we build from the bottom up. Each leaf node represents the result set of a triple pattern, and each intermediate node represents the join of two result sets. We featurize the result sets of leaf nodes by their cardinality, the location and presence of variables, named nodes, and literals in the triple pattern and finally a dense predicate vector representation. We learn the dense vector representations by applying [RDF2Vec](cite:cites ristoski2016rdf2vec) to the input RDF graph. The predicate vectors should thus contain semantic information relating to the encoded predicate. The representations for intermediate joins are obtained by applying a 2-ary tree-LSTM on the result sets involved in the join and using the output hiddenstate and memorycell as representation. Finally, at the current root node, which is the last added join, we apply a child-sim tree LSTM operation to all unjoined result sets to obtain the current join plan representation. \\

The optimizer chooses the next join by adding all possible virtual root nodes to the current join plan. It uses the representation of the virtual root node to predict the Q-value, which represents the expected execution time of the query. The optimizer then chooses the join that minimizes this Q-value and adds it to the join plan. When the join plan is complete we execute the query and record its execution time. The model, which includes the tree-LSTM layers is then trained to minimize the Mean Squared Error (MSE) between the predicted Q-value and the actual query execution time.

### Data efficiency & SPARQL specific adjustments
{:#dataefficiency}
When the model first starts training it is effectively randomly choosing its join plan. This will likely generate very slow execution join plans, slowing down training and hindering performance. To prevent this we apply two data efficiency techniques. Firstly, we include a time-out that is set according to existing optimizers. We effictively truncate our optimization variable such that we are not stuck in bad query plans for too long, while still ensuring that the optimal query plan will not reach the time-out. Secondly, we use [experience replay](cite:cites lin1992self) reuse previous query executions. In experience replay, we store a buffer of the $$N$$ most recent query executions. The execution of queries is the main training bottleneck, thus by applying experience replay we can reuse expensive query executions to further optimize our model. 

The key difference between relational and SPARQL query optimization is the lack of rigid data aggregators in SPARQL, and the usually large number of unique RDF terms present in a RDF graph. To prevent the input vectors from exploding in size we explicitly refrain from creating feature vectors using any form of one-hot encoding. While this complicates creating informative features, we expect the benefit to scalibility to be worth it.

### Open Challenges
{:#openchallenges}
There are key open problems we have not found an elegant solution to. We do not encode the query shape into a preferably fixed size vector. The shape encoding should be related to the different types of connections between result sets, like object-object, subject-subject, object-subject, and subject-object. These different edge types make using a simple adjacency matrix difficult. Furthermore, our approach can only optimize basic graph patterns, in future work we should extend this approach to more complex SPARQL query operations.