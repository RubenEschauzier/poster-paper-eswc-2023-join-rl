## Method
{:#method}
To iteratively build up an optimized join plan, the RL-based optimizer greedily adds the join that minimizes the estimated query execution time at each iteration. For the first iteration, we have the result sets of all triple patterns, and in each subsequent iteration, we join two result sets.
We estimate the execution time of the query using a neural network, which we train to minimize the mean squared error between predicted and actual query execution time. 
The neural network is fed a numerical representation of the current join plan as an input. \\
**Join plan representation** Like in the optimizer [RTOS](cite:cites yu2020reinforcement), we represent join plans as a tree that we build from the bottom up. Each leaf node represents the result set of a triple pattern, and internal nodes represent join result sets. We represent result sets using their cardinality, the presence and location of variables, named nodes and literals, and a vector representation of the predicate.
We learn the predicate representation vectors by applying the [RDF2Vec](cite:cites ristoski2016rdf2vec) algorithm to the RDF graph. We do not encode named nodes in the subject and object positions. RDF2Vec does not provide a straightforward way to learn predicate representations of variables, which are often present in the subject and object positions.
We obtain the representations for intermediate joins by applying an N-ary [Tree-LSTM](cite:cites tai2015improved) neural network on the result sets representations involved in the join. Finally, at the (partial) join plan root node, we apply the Child-Sum Tree-LSTM [network](cite:cites tai2015improved) to all unjoined result sets to obtain the join plan representation.\\
**Data efficiency & SPARQL-specific adjustments** Data generation using query execution is slow; we account for this by applying two data efficiency techniques. First, we include a _time-out_ set according to existing optimizers. We effectively truncate our optimization variable while ensuring the optimal query plan will not reach the time-out. Second, we use [_experience replay_](cite:cites lin1992self) to store previous (expensive) query executions and reuse them for training. \\
Relational RL-based optimization approaches use [_one-hot encoding_](cite:cites muller2016introduction) of database attributes to create feature vectors. However, large graphs like Wikidata can contain over 100 million unique entries. One-hot encoding that many attributes would create unwieldy vectors. To improve scalability, we do not use one-hot encoding in our approach. \\
**Open Challenges** We have not found a way to encode connections between triple patterns. To encode all information in the query graph, these encodings should reflect the possible connections between triple patterns, like object-object, subject-subject, object-subject, and subject-object. Which makes using a simple adjacency matrix infeasible. Furthermore, our approach can only optimize basic graph patterns; in future work, this approach should be extended to more complex SPARQL query operations. Finally, we do not learn feature representations for variables; to enrich our triple pattern representation, we should encode variables based on the other RDF terms in the triple pattern. 