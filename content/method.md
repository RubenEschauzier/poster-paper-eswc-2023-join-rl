## Method
{:#method}
To iteratively build up an optimized join plan, the RL-based optimizer greedily adds the join that minimizes the estimated query execution time at each iteration. For the first iteration, we have the result sets of all triple patterns, and in each subsequent iteration, we join two result sets.
We estimate the execution time of the query using a neural network, which we train to minimize the mean squared error between predicted and actual query execution time. We feed a numerical representation of the current join plan as an input to the neural network. \\
<!-- The neural network is fed a numerical representation of the current join plan as an input. \\ -->
**Join plan representation** Like in the optimizer [RTOS](cite:cites yu2020reinforcement), we represent join plans as a tree that we build from the bottom up. Each leaf node represents the result set of a triple pattern, and internal nodes represent join result sets. We represent result sets using their cardinality, the presence and location of variables, named nodes and literals, and a vector representation of the predicate.
We learn the predicate representation vectors by applying the [RDF2Vec](cite:cites ristoski2016rdf2vec) algorithm to the RDF graph. \\
RDF2Vec generates learned vector representations of RDF terms that encode information on what RDF terms co-occur often. RDF2Vec first generates random walks on the input RDF graph, then for each random walk, it randomly removes an RDF term and trains a neural network to predict the missing term. The weights obtained during the model training are the feature vectors of the RDF terms in the graph. RDF2Vec does not learn variable representations because an RDF graph has no variables. The subject and object of triple patterns are often variables, so we do not encode named nodes in these positions.
<!-- 
RDF2Vec does not provide a straightforward way to learn predicate representations of variables, due to the input RDF graph not containing any variables. Triple patterns often contain variables in the subject and object positions, thus we do not encode named nodes in these positions. -->
<!-- Triple patterns often contain variables in the subject and object positions, thus we do not encode named nodes in these positions. RDF2Vec is applied to the input RDF graph, which does not contain any variable terms, complicating the creation of informative variable representations. -->
<!-- We do not encode named nodes in the subject and object positions. RDF2Vec does not provide a straightforward way to learn predicate representations of variables, which are often present in the subject and object positions.  -->
We obtain the representations for intermediate joins by applying an N-ary [Tree-LSTM](cite:cites tai2015improved) neural network on the result sets representations involved in the join. These representations are optimized during training, thus allowing the model to determine which features of the result sets involved in the join are important. Finally, at the (partial) join plan root node, we apply the Child-Sum Tree-LSTM [network](cite:cites tai2015improved) to all unjoined result sets to obtain the numerical join plan representation.\\
**Data efficiency & SPARQL-specific adjustments** Data generation using query execution is slow; we account for this by applying two data efficiency techniques. First, we include a _time-out_ set according to existing optimizers. We effectively truncate our optimization variable while ensuring the optimal query plan will not reach the time-out. Second, we use [_experience replay_](cite:cites lin1992self) to store previous (expensive) query executions and reuse them for training. \\
Relational RL-based optimization approaches use [_one-hot encoding_](cite:cites muller2016introduction) of database attributes to create feature vectors. However, large graphs like Wikidata can contain over 100 million unique entries. One-hot encoding that many attributes would create unwieldy vectors and degrade performance. To improve scalability, we do not use one-hot encoding in our approach, instead, we use dense feature encoding techniques to capture state information in fixed-size vectors. \\
**Open Challenges** We have not found a way to encode connections between triple patterns. To encode all information in the query graph, these encodings should reflect the possible connections between triple patterns, like object-object, subject-subject, object-subject, and subject-object. Which makes using a simple adjacency matrix infeasible. Furthermore, our approach can only optimize basic graph patterns; in future work, this approach should be extended to more complex SPARQL query operations. Finally, we do not learn feature representations for variables; to enrich our triple pattern representation, we should encode variables based on the other RDF terms in the triple pattern. 