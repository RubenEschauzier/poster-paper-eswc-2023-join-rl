## Method
{:#method}
In this section, we will discuss how our optimizer operates, what methods we use to improve data efficiency and tackle SPARQL-specific problems, and finally what open challenges remain.

### Featurization of (partial) join plans
{:#featurization} 
Like in [RTOS](cite:cites yu2020reinforcement), we represent join plans as a tree structure that we build from the bottom up. Each leaf node represents the result set of a triple pattern, and each intermediate node represents the join of two result sets. We encode the result sets of leaf nodes by concatenating cardinality, the location and presence of variables, named nodes, and literals in the triple pattern, and finally a dense predicate vector representation. We learn the dense vector representations by applying [RDF2Vec](cite:cites ristoski2016rdf2vec) to the input RDF graph. The predicate vectors contain semantic information relating to the encoded predicate. The representations for intermediate joins are obtained by applying an N-ary Tree-LSTM on the result sets involved in the join and using the output as the representation. Finally, at the current root node, which is the last added join, we apply a Child-Sum Tree-LSTM operation to all unjoined result sets obtain the join plan representation.
The optimizer chooses the next join by enumerating over all possible virtual root nodes from the current plan. It predicts the Q-value of the added join based on the virtual node representation. The optimizer then chooses the join that minimizes this Q-value and adds it to the join plan. When the join plan is complete, we execute the query and record its execution time. The model, which includes the tree-LSTM layers is then trained to minimize the Mean Squared Error (MSE) between the predicted Q-value and the actual query execution time.

### Data efficiency & SPARQL specific adjustments
{:#dataefficiency}
When the model first starts training it is effectively randomly choosing its join plan. This will likely generate very slow execution join plans, slowing down training and hindering performance. To prevent this, we apply two data efficiency techniques. Firstly, we include a time-out that is set according to existing optimizers. We effectively truncate our optimization variable such that we are not stuck in bad query plans for too long, while still ensuring that the optimal query plan will not reach the time-out. Secondly, we use [experience replay](cite:cites lin1992self) to reuse previous query executions. In experience replay, we store a buffer of the $$N$$ most recent query executions. The execution of queries is the main training bottleneck, thus by applying experience replay we can reuse expensive query executions to further optimize our model. 

The key difference between relational and SPARQL query optimization is the lack of rigid data aggregators in SPARQL and the often large number of unique RDF terms present in an RDF graph. To prevent the input vectors from exploding in size we explicitly refrain from creating feature vectors using any form of one-hot encoding. While this complicates creating informative features, we expect the improved scalability to offset this disadvantage in the long run.

### Open Challenges
{:#openchallenges}
There are key open problems for which we have not found an elegant solution. Currently, we do not encode the query shape into a, preferably fixed-size, vector. The shape encoding should be related to the different types of connections between result sets, like object-object, subject-subject, object-subject, and subject-object. These different edge types make using a simple adjacency matrix difficult. Furthermore, our approach can only optimize basic graph patterns, in future work we should extend this approach to more complex SPARQL query operations.
